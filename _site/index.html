<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>DiffX</title>
<link href="./styles/style.css" rel="stylesheet">
<script type="text/javascript" src="./styles/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./styles/jquery.js"></script>

<style>
  p.serif{
    font-family:"Times New Roman", Times, serif;
  }
  p.sansserif{
    font-family: Arial, Helvetica, sans-serif;
  }

  h3 {
      text-align: center;
    }

</style>

  
</head>

<body>
<div class="content">

<h1><strong>DiffX: Guide Your Layout to Cross-Modal Generative Modeling</strong>&nbsp;&nbsp;<small></small></h1>
<p id="authors" class="serif">
    <a href="https://github.com/zeyuwang-zju">Zeyu Wang<sup>*</sup><sup>1</sup></a>
    <a href="hhttps://jonyond-lin.github.io/">Jingyu Lin<sup>*</sup><sup>2<dag></sup></a>
    <a href="https://scholar.google.com.hk/citations?user=3hkMMBAAAAAJ&hl=zh-CN&oi=ao">Yifei Qian<sup>3</sup></a>
    <a>Yi Huang<sup>4</sup></a>
    <a>Shicen Tian<sup>1</sup></a> <br>
    <a>Bosong Chai<sup>1</sup></a>
    <a>Juncan Deng<sup>1</sup></a>
    <a>Qu Yang<sup>5</sup></a>
    <a href="https://dulann.github.io/">Lan Du<sup>2</sup></a>
    <a href="https://cunjian.github.io/">Cunjian Chen<sup>2</sup></a>
    <br>
    <a href="https://scholar.google.com/citations?user=T8ThxCwAAAAJ">Yufei Guo<sup>✝</sup><sup>4</sup></a> 
    <a href="https://person.zju.edu.cn/huangkejie">Kejie Huang<sup>✝</sup><sup>1</sup></a> 
    <br>
    <a style="font-size: 0.7em"><sup>*</sup>Equal Contribution.</a> <a style="font-size: 0.7em"><sup>✝</sup>Corresponding Author.</a>
    <br>
    <span style="font-size: 0.8em; margin-top: 0.5em">
      <a><sup>1</sup>Zhejiang University</a>
      <a><sup>2</sup>Monash University</a>
      <a><sup>3</sup>University of Nottingham</a>
      <a><sup>4</sup>Peking University</a>
      <a><sup>5</sup>Wuhan University</a>
    </span>
  </p>

  <font size="+1">
    <p style="text-align: center;" class="serif">
      <a href="https://arxiv.org/abs/2407.15488" target="_blank" style="font-weight: bold;">[Paper]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://github.com/zeyuwang-zju/DiffX" target="_blank" style="font-weight: bold;">[Github]</a>&nbsp;&nbsp;&nbsp;&nbsp;
      <!-- <a href="#bibtex" style="font-weight: bold;">[BibTeX]</a> -->
    </p><br>
  </font>

  <style>
    .row {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      
    }

    
  
    .col {
      flex: 1;
      text-align: center;
     
    }

    .head {
      flex: 1;
      text-align: center;
      
    }

    h3 {
      
      font-size: 15px; /* 缩小标题的大小 */
      
    }
  
  
    img, video {
      width: 100%; /* 将图片和视频的宽度设置为100%，以使它们在同一行上大小相匹配 */
      height: auto;
    }

   

  </style>

<!-- <font size="+1">
    <p class="serif" style="color: red; font-weight: bold">Click to play results from Cinemo!</Summary></p>
</font> -->

<div class="row">
    <div class="col">
      <h3>Layout <br> Image</h3>
      <!-- Layout Image -->
      <img class="case-image" src="static/gallery/results/dual/1-4.jpg" alt="Case 1 Image" style="width: 100%;">
      
    </div>
    <div class="col">
      <h3>Generated <br> RGB Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/1-5.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Generated <br> Infrared Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/1-6.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Layout <br> Image</h3>
      <!-- Layout Image -->
      <img class="case-image" src="static/gallery/results/dual/1-10.jpg" alt="Case 1 Image" style="width: 100%;">
      
    </div>
    <div class="col">
      <h3>Generated <br> RGB Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/1-11.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Generated <br> Infrared Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/1-12.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
  </div>


  <div class="row">
    <div class="col">
      <h3>Layout <br> Image</h3>
      <!-- Layout Image -->
      <img class="case-image" src="static/gallery/results/dual/2-7.jpg" alt="Case 1 Image" style="width: 100%;">
      
    </div>
    <div class="col">
      <h3>Generated <br> RGB Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/2-8.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Generated <br> Infrared Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/2-9.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Layout <br> Image</h3>
      <!-- Layout Image -->
      <img class="case-image" src="static/gallery/results/dual/2-10.jpg" alt="Case 1 Image" style="width: 100%;">
      
    </div>
    <div class="col">
      <h3>Generated <br> RGB Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/2-11.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Generated <br> Infrared Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/2-12.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
  </div>


  <div class="row">
    <div class="col">
      <h3>Layout <br> Image</h3>
      <!-- Layout Image -->
      <img class="case-image" src="static/gallery/results/dual/4-10.jpg" alt="Case 1 Image" style="width: 100%;">
      
    </div>
    <div class="col">
      <h3>Generated <br> RGB Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/4-11.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Generated <br> Depth Map</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/4-12.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Layout <br> Image</h3>
      <!-- Layout Image -->
      <img class="case-image" src="static/gallery/results/dual/4-7.jpg" alt="Case 1 Image" style="width: 100%;">
      
    </div>
    <div class="col">
      <h3>Generated <br> RGB Image</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/4-8.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
    <div class="col">
      <h3>Generated <br> Depth Map</h3>
      <!-- Case 1 Image -->
      <img class="case-image" src="static/gallery/results/dual/4-9.jpg" alt="Case 1 Image" style="width: 100%;">
    </div>
  </div>

</p>

</div>







<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Cross-Modal Dataset Construction</p>
  <div class="container is-max-desktop">
      <div class="hero-body">
          <center>
                <img src="static/figs/Workflow.png" title="" style="max-width:100%;vertical-align:top"/>
          </center>
      </div>
    </div>
  
  <p style="font-size: 1.4em" class="serif">
    (a) The process of constructing the image captions. (b) Examples of cross-modal images, labels, and prepared captions.
</div>






<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Workflow</p>
  <p style="font-size: 1.4em" class="serif">
    Workflow of our Multi-Path Variational AutoEncoder (MP-VAE). Here, the RGB+X modal encoding is employed for illustration. However, the framework is capable
of supporting additional modal inputs and outputs.
  </p>
  <!-- <font size="+1">
    <p class="serif" style="color: red; font-weight: bold">Click to play the following animations!</p>
  </font> -->
<!--  <span style="color: red; font-weight: bold"></span>-->
  <!-- <img class="summary-img" src="figs/framework.jpg" style="width:100%;"> <br> -->

  <div class="row">
    <div class="col-gallery" style="width: 45%;">
      <img src="static/figs/mpvae.png" style="width: 90%;">
      <h3>MP-VAE</h3>
    </div>
    <div class="col-gallery" style="width: 45%;">
      <img src="static/figs/worklatent.png" style="width: 90%;">
      <h3>Workflow in Latent Space</h3>
    </div>

  </div>

</div>










<div class="content">
    <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Methodology</p>
    <div class="container is-max-desktop">
        <div class="hero-body">
            <center>
                  <img src="static/figs/pipeline.png" title="" style="max-width:90%;vertical-align:top"/>
            </center>
        </div>
      </div>
    
    <p style="font-size: 1.4em" class="serif">
      Diffusion models have made significant strides in language-driven and layout-driven image generation.
      However, most diffusion models are limited to visible RGB image generation.
      In fact, human perception of the world is enriched by diverse viewpoints, such as chromatic contrast, thermal illumination, and depth information.
      In this paper, we introduce a novel diffusion model for general layout-guided cross-modal generation, called DiffX.
      Notably, our DiffX presents a simple yet effective cross-modal generative modeling pipeline, which conducts diffusion and denoising processes in the modality-shared latent space.
      Moreover, we introduce the Joint-Modality Embedder (JME) to enhance the interaction between layout and text conditions by incorporating a gated attention mechanism.
      To facilitate the user-instructed training, we construct the cross-modal image datasets with detailed text captions by the Large-Multimodal Model (LMM) and our human-in-the-loop refinement.
      Through extensive experiments, our DiffX demonstrates robustness in cross-modal ``RGB+X'' image generation on FLIR, MFNet, and COME15K datasets, guided by various layout conditions.
      It also shows the potential for the adaptive generation of ``RGB+X+Y(+Z)'' images or more diverse modalities on COME15K and MCXFace datasets.
</div>





<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Comparisons</p>
  <p style="font-size: 1.4em" class="serif">
    As shown in the figure below, we can see that our DiffX significantly outperforms the baseline methods in qualitative results. It proves the
    effectiveness of employing Long-CLIP for caption embedding and our JME for joint-modal connection.<br> <br>
    And results in the table below demonstrate that our DiffX outperforms all baseline models in all metrics.
  </p>
  <!-- <font size="+1">
    <p class="serif" style="color: red; font-weight: bold">Click to play the following animations!</p>
  </font> -->
<!--  <span style="color: red; font-weight: bold"></span>-->
  <!-- <img class="summary-img" src="figs/framework.jpg" style="width:100%;"> <br> -->

  <div class="row">
    <div class="col-gallery" style="width: 43%;">
      <img src="static/comparisons/Comparison.png" style="width: 95%;">
      <h3>Qualitative Comparison</h3>
    </div>
    <div class="col-gallery" style="width: 45%;">
      <img src="static/comparisons/compare_table.png" style="width: 95%;">
      <h3>Quantitative Comparison</h3>
    </div>

  </div>

</div>

  <!-- <hr style="margin: 30px 0;"> -->




<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Analysis</p>
  <p style="font-size: 1.4em" class="serif">
    The ablation studies and potential applications are presented here.
  </p>
  <p style="text-align:left; font-size: 1.6em; font-weight: bold" class="serif">Ablation Study</p>
  <p style="font-size: 1.4em" class="serif">
    Firstly, we conduct the ablation study on the Laplacian Pyramid (LP) in our MP-VAE. Secondly, we aim to compare the unique-modal generation with the cross-modal RGB+X generation by DiffX. 
  </p>


  <div class="row">
    <div class="col-gallery" style="width: 50%;">
      <img src="static/gallery/results/LP.png" style="width: 95%;">
      <h3>Ablation study on the Laplacian Pyramid</h3>
    </div>
    <div class="col-gallery" style="width: 40%;">
      <img src="static/gallery/results/single-to-dual.png" style="width: 95%;">
      <h3>Comparison between the unique- and cross-modal</h3>
    </div>
  </div>


  <hr style="margin: 30px 0;">

  <!-- <hr style="height: 2px; background-color: #000; border: none; margin: 30px 0;"> -->


  <p style="text-align:left; font-size: 1.6em; font-weight: bold" class="serif">Effectiveness of long text captions</p>
  <p style="font-size: 1.4em" class="serif">
    We conduct the ablation study on the impact of text captions on SOD → RGB+D and Seg. → RGB+T tasks. The qualitative comparison in figure below shows that DiffX can effectively capture the crucial captions, while the variant model without caption embeddings generates broken or misaligned images, ultimately affecting the image quality.
  </p>

  <div class="row">
    <div class="col-gallery" style="width: 70%;">
      <img src="static/gallery/results/caption_impact.png" style="width: 100%;">
      <h3>Impact of text captions in cross-modal generation</h3>
    </div>
  </div>


  <hr style="margin: 30px 0;">

  <p style="text-align:left; font-size: 1.6em; font-weight: bold" class="serif">Adaptation to Diverse-Modal Generation</p>
  <p style="font-size: 1.4em" class="serif">
    Given that DiffX’s can generate cross-modal “RGB+X” images, we also wonder if we can apply this framework to robust, controllable, and versatile generation across diverse modalities? 
    Therefore, we also conduct experiments on COME15K and MCXFace datasets for “SOD → RGB+D+Edge” and “3DDFA → RGB+NIR+SWIR+T”, respectively.
    The qualitative results are shown in the figures below, respectively. 
  </p>

  <div class="row">
    <div class="col-gallery" style="width: 35%;">
      <img src="static/gallery/results/triple.png" style="width: 95%;">
      <h3>Adaptation to “SOD→RGB+D+Edge”</h3>
    </div>
    <div class="col-gallery" style="width: 58%;">
      <img src="static/gallery/results/four_modal.png" style="width: 95%;">
      <h3>Adaptation to “3DDFA→RGB+NIR+SWIR+T”</h3>
    </div>
  </div>


</div>

<div class="content">
  <p style="text-align:left; font-size: 2em; font-weight: bold" class="serif">Gallery</p>
  <p style="font-size: 1.4em" class="serif">
      More results generated by our method are shown here.<br>
  </p>
  <!-- <font size="+1">
    <p class="serif" style="color: red; font-weight: bold">Click to play results from Cinemo!</p>
  </font> -->
  <!-- <img class="summary-img" src="figs/framework.jpg" style="width:100%;"> <br> -->
  <div class="row">
    <div class="col-gallery" style="width: 80%;">
      <img src="static/my_gallery/result2.png" style="width: 100%;">
      <h3>“SOD → RGB+D+Edge” task</h3>
    </div>
    <div class="col-gallery" style="width: 80%;">
      <img src="static/my_gallery/result3.png" style="width: 100%;">
      <h3>“3DDFA → RGB+NIR+SWIR+T” task</h3>
    </div>
  </div>

  </div>
 
</div>


<!-- <div class="content">
  <p class="serif">
    Project page template is borrowed from <a href="https://dreambooth.github.io/">DreamBooth</a>.
  </p>
</div> -->

</body>

<script>
var videos = document.getElementsByClassName("clickplay");
for (var i = 0; i < videos.length; i++) {
  videos[i].addEventListener("click", function() {
    this.play();
  });
  videos[i].addEventListener("ended", function() {
    this.pause();
    this.currentTime = 0;
  });
}
</script>

</html>
